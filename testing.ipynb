{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import sklearn\n",
    "#import tensorflow\n",
    "\n",
    "import tensorflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM ,Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic #makes detections\n",
    "mp_drawing = mp.solutions.drawing_utils #draws detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"model_new_final.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False   #image not writeable\n",
    "    results = model.process(image)  #make prediction\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                                                                                            mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1))\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "                                                                                            mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1))\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "                                                                                            mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "\n",
    "    pose=pose[:69]\n",
    "    return np.concatenate([pose,lh,rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_Path = os.path.join(\"test video_all/test video\")\n",
    "#Data_Path = os.path.join(\"test video\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = np.array([\"Book\",\"Do\",\"Eat\",\"Go\",\"Good\",\"Hello\",\"Home\",\"Hungry\",\"I\",\"Morning\",\"No\",\"Not\",\"Pizza\" , \"Place\" ,\"Read\",\"School\",\"Student\",\"Teacher\",\"Thank You\", \"This\" , \"Tomorrow\" ,\"Want\", \"What\", \"Yes\", \"Yesterday\",\"You\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num , label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = \"Place\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sequence in range(length):\n",
    "        try:\n",
    "            os.makedirs(os.path.join(Data_Path,action, (str(sequence))))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        \n",
    "    for sequence in range(length):\n",
    "        \n",
    "        for frame_num in range(30):\n",
    "\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            image, results = mediapipe_detection(frame, holistic)\n",
    "                            \n",
    "            draw_landmarks(image, results)\n",
    "                        \n",
    "            if frame_num == 0: \n",
    "                cv2.putText(image, 'STARTING COLLECTION', (120,200), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                cv2.putText(image, 'Collecting Video Number {}'.format(sequence), (15,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                \n",
    "                cv2.imshow('OpenCV Feed', image)\n",
    "                cv2.waitKey(1500)\n",
    "            else: \n",
    "                cv2.putText(image, 'Collecting Video Number {}'.format(sequence), (15,12), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                \n",
    "                cv2.imshow('OpenCV Feed', image)               \n",
    "            \n",
    "            keypoints = extract_keypoints(results)\n",
    "\n",
    "            npy_path = os.path.join(Data_Path, action , (str(sequence)), str(frame_num))\n",
    "            np.save(npy_path, keypoints)\n",
    "\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "                    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_keypoints(keypoints, center_keypoint, reference_distance):\n",
    "    # Reshape the keypoints into (x, y, z) coordinates\n",
    "    #print(keypoints)\n",
    "    keypoints = keypoints.reshape(-1, 3)\n",
    "    \n",
    "    #print(keypoints)\n",
    "    # Subtract center keypoint to get relative coordinates\n",
    "    relative_keypoints = keypoints - center_keypoint\n",
    "\n",
    "    #print(relative_keypoints)\n",
    "    \n",
    "    # If reference distance is provided, scale the keypoints\n",
    "    relative_keypoints = relative_keypoints / reference_distance\n",
    "    #relative_keypoints = keypoints / reference_distance\n",
    "    \n",
    "\n",
    "    return relative_keypoints.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' sequences = []\\n\\n\\nfor sequence in range(length):\\n    window = []\\n    for frame_num in range(30):\\n        frame = np.load(os.path.join(Data_Path, action,str(sequence), f\"{frame_num}.npy\"))\\n        center_keypoint = frame[0:3]  # Nose keypoint (x, y, z)\\n        left_shoulder = frame[11*3:11*3+3]\\n        right_shoulder = frame[12*3:12*3+3]\\n        reference_distance = np.linalg.norm(left_shoulder - right_shoulder)\\n        if not reference_distance:\\n            reference_distance=1\\n        \\n\\n        frame = normalize_keypoints(frame, center_keypoint, reference_distance)\\n        window.append(frame)\\n    sequences.append(window) '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = []\n",
    "\n",
    "\n",
    "for sequence in range(length):\n",
    "    window = []\n",
    "    for frame_num in range(30):\n",
    "        frame = np.load(os.path.join(Data_Path, action,str(sequence), f\"{frame_num}.npy\"))\n",
    "        center_keypoint = frame[0:3]  # Nose keypoint (x, y, z)\n",
    "        left_shoulder = frame[11*3:11*3+3]\n",
    "        right_shoulder = frame[12*3:12*3+3]\n",
    "        reference_distance = np.linalg.norm(left_shoulder - right_shoulder)\n",
    "        if not reference_distance:\n",
    "            reference_distance=1\n",
    "        \n",
    "\n",
    "        frame = normalize_keypoints(frame, center_keypoint, reference_distance)\n",
    "        window.append(frame)\n",
    "    sequences.append(window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import uniform_filter1d  # For temporal smoothing\n",
    "\n",
    "# Function to calculate relative hand keypoints\n",
    "def preprocess_hand_keypoints(hand_keypoints):\n",
    "    # If there are hand keypoints, calculate relative positions with respect to the wrist (0th keypoint)\n",
    "    if np.any(hand_keypoints):\n",
    "        wrist_keypoint = hand_keypoints[0:3]  # Wrist is the first keypoint in MediaPipe\n",
    "        relative_hand_keypoints = (hand_keypoints.reshape(-1, 3) - wrist_keypoint)  # Relative to wrist\n",
    "    else:\n",
    "        relative_hand_keypoints = np.zeros(21 * 3)  # If no hand keypoints, return zeros\n",
    "    return relative_hand_keypoints.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n\\n\\n# Preprocessing: Including relative hand keypoints and temporal smoothing\\nsequences, labels = [], []\\n\\n\\nfor sequence in range(length):\\n    window = []\\n    for frame_num in range(30):\\n        frame = np.load(os.path.join(Data_Path,action,str(sequence), f\"{frame_num}.npy\"))\\n        \\n        # Center keypoint (nose) and shoulder distance (for normalization)\\n        center_keypoint = frame[0:3]  # Nose keypoint (x, y, z)\\n        left_shoulder = frame[11*3:11*3+3]\\n        right_shoulder = frame[12*3:12*3+3]\\n        reference_distance = np.linalg.norm(left_shoulder - right_shoulder)\\n        if not reference_distance:\\n            reference_distance = 1\\n        \\n        # Normalize the pose keypoints relative to the nose\\n        normalized_pose = normalize_keypoints(frame[:69], center_keypoint, reference_distance)\\n        \\n        # Preprocess left hand keypoints (relative to wrist)\\n        left_hand = frame[69:69 + 21*3]\\n        relative_left_hand = preprocess_hand_keypoints(left_hand)\\n        \\n        # Preprocess right hand keypoints (relative to wrist)\\n        right_hand = frame[69 + 21*3:]\\n        relative_right_hand = preprocess_hand_keypoints(right_hand)\\n        \\n        # Concatenate normalized pose, relative left hand, and relative right hand keypoints\\n        full_frame = np.concatenate([normalized_pose, relative_left_hand, relative_right_hand])\\n        \\n        window.append(full_frame)\\n    \\n    # Convert the window into a numpy array for smoothing\\n    window = np.array(window)\\n    \\n    # Apply temporal smoothing using a moving average filter\\n    smoothed_window = uniform_filter1d(window, size=3, axis=0)\\n    \\n    sequences.append(smoothed_window)\\n\\n '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "\n",
    "\n",
    "# Preprocessing: Including relative hand keypoints and temporal smoothing\n",
    "sequences, labels = [], []\n",
    "\n",
    "\n",
    "for sequence in range(length):\n",
    "    window = []\n",
    "    for frame_num in range(30):\n",
    "        frame = np.load(os.path.join(Data_Path,action,str(sequence), f\"{frame_num}.npy\"))\n",
    "        \n",
    "        # Center keypoint (nose) and shoulder distance (for normalization)\n",
    "        center_keypoint = frame[0:3]  # Nose keypoint (x, y, z)\n",
    "        left_shoulder = frame[11*3:11*3+3]\n",
    "        right_shoulder = frame[12*3:12*3+3]\n",
    "        reference_distance = np.linalg.norm(left_shoulder - right_shoulder)\n",
    "        if not reference_distance:\n",
    "            reference_distance = 1\n",
    "        \n",
    "        # Normalize the pose keypoints relative to the nose\n",
    "        normalized_pose = normalize_keypoints(frame[:69], center_keypoint, reference_distance)\n",
    "        \n",
    "        # Preprocess left hand keypoints (relative to wrist)\n",
    "        left_hand = frame[69:69 + 21*3]\n",
    "        relative_left_hand = preprocess_hand_keypoints(left_hand)\n",
    "        \n",
    "        # Preprocess right hand keypoints (relative to wrist)\n",
    "        right_hand = frame[69 + 21*3:]\n",
    "        relative_right_hand = preprocess_hand_keypoints(right_hand)\n",
    "        \n",
    "        # Concatenate normalized pose, relative left hand, and relative right hand keypoints\n",
    "        full_frame = np.concatenate([normalized_pose, relative_left_hand, relative_right_hand])\n",
    "        \n",
    "        window.append(full_frame)\n",
    "    \n",
    "    # Convert the window into a numpy array for smoothing\n",
    "    window = np.array(window)\n",
    "    \n",
    "    # Apply temporal smoothing using a moving average filter\n",
    "    smoothed_window = uniform_filter1d(window, size=3, axis=0)\n",
    "    \n",
    "    sequences.append(smoothed_window)\n",
    "\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book 0.580436\n",
      "Book 0.93534654\n",
      "Book 0.99996626\n",
      "Do 0.99912757\n",
      "Do 0.9999665\n",
      "Do 0.99998283\n",
      "Eat 0.9564631\n",
      "Eat 0.998604\n",
      "Eat 0.9999187\n",
      "Good 0.9508843\n",
      "Go 0.9999794\n",
      "Go 0.99953735\n",
      "Good 0.99978393\n",
      "Good 0.9959163\n",
      "Good 0.9997267\n",
      "Hello 0.9999902\n",
      "Hello 0.9994954\n",
      "Hello 0.9956357\n",
      "Home 0.9999206\n",
      "Home 0.99992955\n",
      "Home 0.99991214\n",
      "Hungry 0.99584997\n",
      "Hungry 0.9980263\n",
      "Hungry 0.9978848\n",
      "I 0.8527386\n",
      "I 0.9985366\n",
      "I 0.9975884\n",
      "Morning 0.86046827\n",
      "Morning 0.9828462\n",
      "Morning 0.97989094\n",
      "No 0.9998191\n",
      "No 0.9998306\n",
      "No 0.9998073\n",
      "Not 0.99982136\n",
      "Not 0.99985206\n",
      "Not 0.99960655\n",
      "Pizza 0.9881457\n",
      "Pizza 0.9996012\n",
      "Pizza 0.8172637\n",
      "Place 0.99997604\n",
      "Place 0.99999917\n",
      "Place 0.9999987\n",
      "Read 0.99999785\n",
      "Read 0.99999857\n",
      "Read 0.9999969\n",
      "School 0.99996567\n",
      "School 0.99996364\n",
      "School 0.9999684\n",
      "Student 0.999813\n",
      "Student 0.998547\n",
      "Student 0.99992955\n",
      "Teacher 0.999995\n",
      "Teacher 0.99999595\n",
      "Teacher 0.99999535\n",
      "Thank You 0.99993086\n",
      "Thank You 0.99991226\n",
      "Thank You 0.99984455\n",
      "This 0.99996793\n",
      "This 0.99954766\n",
      "This 0.9999707\n",
      "Tomorrow 0.9999801\n",
      "Tomorrow 0.9999664\n",
      "Tomorrow 0.99999595\n",
      "Want 0.9995746\n",
      "Want 0.99935\n",
      "Want 0.9959636\n",
      "What 0.9999249\n",
      "What 0.9999572\n",
      "What 0.99273634\n",
      "Yes 0.9988709\n",
      "Yes 0.99069875\n",
      "Yes 0.9990326\n",
      "Yesterday 0.99963284\n",
      "Yesterday 0.9988105\n",
      "Yesterday 0.9995913\n",
      "You 0.999757\n",
      "You 0.99978524\n",
      "You 0.9988575\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Preprocessing: Including relative hand keypoints and temporal smoothing\n",
    "sequences, labels = [], []\n",
    "\n",
    "for action in actions:\n",
    "    sequences, labels = [], []\n",
    "    for sequence in range(length):\n",
    "        window = []\n",
    "        for frame_num in range(30):\n",
    "            frame = np.load(os.path.join(Data_Path,action,str(sequence), f\"{frame_num}.npy\"))\n",
    "            \n",
    "            # Center keypoint (nose) and shoulder distance (for normalization)\n",
    "            center_keypoint = frame[0:3]  # Nose keypoint (x, y, z)\n",
    "            left_shoulder = frame[11*3:11*3+3]\n",
    "            right_shoulder = frame[12*3:12*3+3]\n",
    "            reference_distance = np.linalg.norm(left_shoulder - right_shoulder)\n",
    "            if not reference_distance:\n",
    "                reference_distance = 1\n",
    "            \n",
    "            # Normalize the pose keypoints relative to the nose\n",
    "            normalized_pose = normalize_keypoints(frame[:69], center_keypoint, reference_distance)\n",
    "            \n",
    "            # Preprocess left hand keypoints (relative to wrist)\n",
    "            left_hand = frame[69:69 + 21*3]\n",
    "            relative_left_hand = preprocess_hand_keypoints(left_hand)\n",
    "            \n",
    "            # Preprocess right hand keypoints (relative to wrist)\n",
    "            right_hand = frame[69 + 21*3:]\n",
    "            relative_right_hand = preprocess_hand_keypoints(right_hand)\n",
    "            \n",
    "            # Concatenate normalized pose, relative left hand, and relative right hand keypoints\n",
    "            full_frame = np.concatenate([normalized_pose, relative_left_hand, relative_right_hand])\n",
    "            \n",
    "            window.append(full_frame)\n",
    "        \n",
    "        # Convert the window into a numpy array for smoothing\n",
    "        window = np.array(window)\n",
    "        \n",
    "        # Apply temporal smoothing using a moving average filter\n",
    "        smoothed_window = uniform_filter1d(window, size=3, axis=0)\n",
    "        \n",
    "        sequences.append(smoothed_window)\n",
    "\n",
    "    ans = \"\"\n",
    "\n",
    "    for sign in range(length):\n",
    "        res = model.predict(np.expand_dims(sequences[sign], axis=0))\n",
    "        ans = ans + str(actions[np.argmax(res)]) + \" \"\n",
    "        ans = ans.upper()\n",
    "        print(actions[np.argmax(res)] , np.max(res))\n",
    "        #print(res)\n",
    "\n",
    "    #print(ans)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 30, 195)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(sequences).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Place 0.99863917\n",
      "Place 0.9999423\n",
      "Place 0.99993014\n",
      "PLACE PLACE PLACE \n"
     ]
    }
   ],
   "source": [
    "ans = \"\"\n",
    "\n",
    "for sign in range(length):\n",
    "    res = model.predict(np.expand_dims(sequences[sign], axis=0))\n",
    "    ans = ans + str(actions[np.argmax(res)]) + \" \"\n",
    "    ans = ans.upper()\n",
    "    print(actions[np.argmax(res)] , np.max(res))\n",
    "    #print(res)\n",
    "\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You (99, 98, 85)-> make sure you point at/above chest level (otherwise can misclassify as \"This\")\n",
    "Yesterday (99,98,99)\n",
    "Yes (99) ->Fast movement as wrist needs to be rotated\n",
    "What (99)\n",
    "Want (99)\n",
    "Tomorrow (99) -> See the video before signing, it helps\n",
    "This (99)\n",
    "Thank You (99)\n",
    "Teacher (99)\n",
    "Student (89, 45 , 99) (Sometimes \"Hungry\", so check sign video)\n",
    "School (99)\n",
    "Read (99)\n",
    "Place (X) ->  misclassified as Teacher \n",
    "\n",
    "\n",
    "\n",
    "model new-\n",
    "Student -> Hungry (1/3)\n",
    "Teacher -> Place (3/3)\n",
    "Yesterday-> Yes(1/3)\n",
    "You -> Yes (1/3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello, good morning, teacher.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate, FewShotPromptTemplate\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "\tbase_url=\"https://api-inference.huggingface.co/v1/\",\n",
    "\tapi_key=\"\"                                                  #enter api key\n",
    ")\n",
    "\n",
    "raw = \"YOU WANT EAT\"\n",
    "raw = \"STUDENT TOMORROW SCHOOL READ BOOK BOOK\"\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an AI skilled at translating raw sign language input into grammatically correct English sentences. Remember that when a word is repeated twice, it means that the word is in plural form not that it is 2 in quantity.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Translate the following sign language into proper English sentences.\"},\n",
    "    \n",
    "    {\"role\": \"assistant\", \"content\": \"Raw Input: 'HOME RAIN HEAVY'\\nTranslation: 'It is raining heavily in my home area'\"},\n",
    "\n",
    "     {\"role\": \"assistant\", \"content\": \"Raw Input: 'I TOMORROW EAT FRUIT FRUIT'\\nTranslation: 'Tomorrow I will eat fruits.'\"},\n",
    "    \n",
    "    {\"role\": \"assistant\", \"content\": \"Raw Input: 'CLASS STUDENTS SIT'\\nTranslation: 'There are students sitting in the class.'\"},\n",
    "    \n",
    "    {\"role\": \"assistant\", \"content\": \"Raw Input: 'I TONIGHT HOME GO LATE'\\nTranslation: 'I will go home late tonight.'\"},\n",
    "\n",
    "    {\"role\": \"assistant\", \"content\": \"Raw Input: 'YOU HUNGRY'\\nTranslation: 'Are you feeling hungry?'\"},\n",
    "    \n",
    "    {\"role\": \"user\", \"content\": f\"Raw Input: {raw}\"},\n",
    "]\n",
    "\n",
    "\n",
    "llm = client.chat.completions.create(\n",
    "    model=\"meta-llama/Meta-Llama-3-8B-Instruct\", \n",
    "\tmessages=messages, \n",
    "\tmax_tokens=20\n",
    ")\n",
    "\n",
    "final = str((llm.choices[0].message.content))\n",
    "print(final)\n",
    "\n",
    "from gtts import gTTS\n",
    "\n",
    "import os\n",
    "\n",
    "language = 'en'\n",
    "\n",
    "\n",
    "myobj = gTTS(text=final, lang=language, slow=False)\n",
    "\n",
    "\n",
    "myobj.save(\"welcome.mp3\")\n",
    "\n",
    "\n",
    "os.system(\"start welcome.mp3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Book 0.99619746\n",
    "Book 0.9999957\n",
    "Book 0.99999547\n",
    "\n",
    "Do 0.98608255\n",
    "Do 0.9999471\n",
    "Do 0.99994814\n",
    "\n",
    "Eat 0.99525696\n",
    "Eat 0.9999442\n",
    "Eat 0.9999498\n",
    "\n",
    "Go 0.99149173\n",
    "Go 0.99988604\n",
    "Go 0.99987805\n",
    "\n",
    "Good -> I\n",
    "\n",
    "Hello 0.99971336\n",
    "Hello 0.99986565\n",
    "Hello 0.9998723\n",
    "\n",
    "Home 0.99402666\n",
    "Home 0.9989083\n",
    "Home 0.99521095\n",
    "\n",
    "Hungry 0.98818445\n",
    "Hungry 0.9999211\n",
    "Hungry 0.99994934\n",
    "\n",
    "I 0.8913519\n",
    "I 0.99081624\n",
    "I 0.52419513\n",
    "\n",
    "Morning -> Do 0.7095311\n",
    "Morning 0.9977514\n",
    "Morning 0.99484104\n",
    "\n",
    "No 0.99256283\n",
    "No 0.99942327\n",
    "No 0.9998179\n",
    "\n",
    "Not ->Tomorrow , You , I\n",
    "\n",
    "Pizza 0.98818153\n",
    "Pizza 0.9998086\n",
    "Pizza 0.9991148\n",
    "\n",
    "Place 0.99283713\n",
    "Place 0.9999306\n",
    "Place 0.9960641\n",
    "\n",
    "Read 0.9882073\n",
    "Read 0.9999994\n",
    "Read 0.9999994\n",
    "\n",
    "School 0.9965444\n",
    "School 0.99957186\n",
    "School 0.9998172\n",
    "\n",
    "Student 0.98731625\n",
    "Student 0.99818146\n",
    "Student 0.535804\n",
    "\n",
    "Teacher 0.9144656\n",
    "Teacher 0.9998091\n",
    "Teacher 0.9999212\n",
    "\n",
    "Thank You 0.9967784\n",
    "Thank You 0.99974054\n",
    "Thank You 0.99701047\n",
    "\n",
    "This 0.99154836\n",
    "This 0.98547655\n",
    "This 0.89454705\n",
    "\n",
    "Tomorrow 0.8973631\n",
    "Tomorrow 0.90961677\n",
    "Tomorrow 0.89528126\n",
    "\n",
    "Want 0.9954282\n",
    "Want 0.98475176\n",
    "Want 0.99558383\n",
    "\n",
    "What 0.99124473\n",
    "What 0.9941282\n",
    "What 0.99150753\n",
    "\n",
    "Yes 0.98449403\n",
    "Yes 0.9991202\n",
    "Yes 0.9855783\n",
    "\n",
    "Yesterday 0.7214897\n",
    "Yes 0.60461485\n",
    "Yesterday 0.9743711\n",
    "\n",
    "You 0.9918717\n",
    "You 0.9999629\n",
    "You 0.9998995\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
