{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data Aquisition w/o Face and Normalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import sklearn\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM ,Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic #makes detections\n",
    "mp_drawing = mp.solutions.drawing_utils #draws detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False   #image not writeable\n",
    "    results = model.process(image)  #make prediction\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                                                                                            mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1))\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "                                                                                            mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1))\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "                                                                                            mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_keypoints(keypoints, center_keypoint, reference_distance):\n",
    "    # Reshape the keypoints into (x, y, z) coordinates\n",
    "    keypoints = keypoints.reshape(-1, 3)\n",
    "    \n",
    "    # Subtract center keypoint to get relative coordinates\n",
    "    relative_keypoints = keypoints - center_keypoint\n",
    "    \n",
    "    # If reference distance is provided, scale the keypoints\n",
    "    relative_keypoints = relative_keypoints / reference_distance\n",
    "    #relative_keypoints = keypoints / reference_distance\n",
    "    \n",
    "\n",
    "    return relative_keypoints.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "\n",
    "    pose=pose[:69]\n",
    "    \n",
    "    return np.concatenate([pose,lh,rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_Path = os.path.join(\"ISL_Data_Prashant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = np.array([\"Not\"])\n",
    "no_sequences = 50  #60 videos\n",
    "sequence_length = 30    #30 frames each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        try:\n",
    "            os.makedirs(os.path.join(Data_Path, action, str(sequence)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    for action in actions:\n",
    "        \n",
    "        for sequence in range(20,50):\n",
    "            \n",
    "            for frame_num in range(sequence_length):\n",
    "\n",
    "                ret, frame = cap.read()\n",
    "\n",
    "                image, results = mediapipe_detection(frame, holistic)\n",
    "                              \n",
    "                draw_landmarks(image, results)\n",
    "                            \n",
    "                if frame_num == 0: \n",
    "                    cv2.putText(image, 'STARTING COLLECTION', (120,200), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    \n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                    cv2.waitKey(2000)\n",
    "                else: \n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    \n",
    "                    cv2.imshow('OpenCV Feed', image)               \n",
    "                \n",
    "                keypoints = extract_keypoints(results)\n",
    "                npy_path = os.path.join(Data_Path, action, str(sequence), str(frame_num))\n",
    "                np.save(npy_path, keypoints)\n",
    "\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "                    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = np.array([\"Father\", \"What\",\"Why\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num , label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Father': 0, 'What': 1, 'Why': 2}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' np.array(sequences).shape   #2 videos * 2 classes , 30 frames each , 225 keypoints in each frame '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" np.array(sequences).shape   #2 videos * 2 classes , 30 frames each , 225 keypoints in each frame \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' np.array(labels).shape '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" np.array(labels).shape \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "\n",
    "def reconstruct_keypoints(flattened_array):\n",
    "    # Number of landmarks and features\n",
    "    num_pose_landmarks = 23\n",
    "    \n",
    "    num_hand_landmarks = 21\n",
    "    num_pose_features = 3\n",
    "    num_hand_features = 3\n",
    "   \n",
    "    \n",
    "    # Determine the indices for slicing\n",
    "    pose_end = num_pose_landmarks * num_pose_features\n",
    "    \n",
    "    lh_end = pose_end + num_hand_landmarks * num_hand_features\n",
    "    rh_end = lh_end + num_hand_landmarks * num_hand_features\n",
    "\n",
    "    # Extract landmarks from flattened array\n",
    "    pose_flat = flattened_array[:pose_end]\n",
    "    \n",
    "    lh_flat = flattened_array[pose_end:lh_end]\n",
    "    rh_flat = flattened_array[lh_end:rh_end]\n",
    "\n",
    "    # Reshape back to original format\n",
    "    pose = np.reshape(pose_flat, (num_pose_landmarks, num_pose_features))\n",
    "    lh = np.reshape(lh_flat, (num_hand_landmarks, num_hand_features))\n",
    "    rh = np.reshape(rh_flat, (num_hand_landmarks, num_hand_features))\n",
    "\n",
    "    # Convert to list of tuples\n",
    "    pose_landmarks = [{'x': pose[i, 0], 'y': pose[i, 1], 'z': pose[i, 2], } for i in range(num_pose_landmarks)]\n",
    "    lh_landmarks = [{'x': lh[i, 0], 'y': lh[i, 1], 'z': lh[i, 2]} for i in range(num_hand_landmarks)]\n",
    "    rh_landmarks = [{'x': rh[i, 0], 'y': rh[i, 1], 'z': rh[i, 2]} for i in range(num_hand_landmarks)]\n",
    "\n",
    "    return pose_landmarks, lh_landmarks, rh_landmarks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks_on_black(image_shape, pose_landmarks, lh_landmarks, rh_landmarks):\n",
    "    \"\"\"Draw landmarks on a black background.\"\"\"\n",
    "    # Create a black image\n",
    "    image = np.zeros(image_shape, dtype=np.uint8)\n",
    "    \n",
    "    # Helper function to draw landmarks\n",
    "    def draw_landmark_list(landmarks, color):\n",
    "        for landmark in landmarks:\n",
    "            x = int(landmark['x'] * image_shape[1])\n",
    "            y = int(landmark['y'] * image_shape[0])\n",
    "            cv2.circle(image, (x, y), 3, color, -1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Draw pose landmarks\n",
    "    if pose_landmarks:\n",
    "        draw_landmark_list(pose_landmarks, (255, 0, 0))  # Red for pose\n",
    "    \n",
    "    # Draw left hand landmarks\n",
    "    if lh_landmarks:\n",
    "        draw_landmark_list(lh_landmarks, (255, 255, 0))  # Yellow for left hand\n",
    "    \n",
    "    # Draw right hand landmarks\n",
    "    if rh_landmarks:\n",
    "        draw_landmark_list(rh_landmarks, (0, 255, 255))  # Cyan for right hand\n",
    "    \n",
    "    return image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape = (500, 500, 3)\n",
    "for j in range(40,60):\n",
    "    for i in range(30):\n",
    "        \n",
    "        path = f\"ISL_Data_Shubham/I/{j}\"\n",
    "        #path = f\"test video/{j}\"\n",
    "        frame = np.load(path+f\"/{i}.npy\")\n",
    "        \n",
    "        center_keypoint = frame[0:3]  # Nose keypoint (x, y, z)\n",
    "    \n",
    "        left_shoulder = frame[11*3:11*3+3]\n",
    "        right_shoulder = frame[12*3:12*3+3]\n",
    "        reference_distance = np.linalg.norm(left_shoulder - right_shoulder)\n",
    "        if not reference_distance:\n",
    "            reference_distance=1\n",
    "        # Normalize pose, left hand, and right hand keypoints with respect to the common keypoint\n",
    "        #frame = normalize_keypoints(frame, center_keypoint, reference_distance)\n",
    "\n",
    "        pose_landmarks, lh_landmarks, rh_landmarks = reconstruct_keypoints(frame)\n",
    "\n",
    "        plots = draw_landmarks_on_black(image_shape, pose_landmarks, lh_landmarks, rh_landmarks)\n",
    "\n",
    "        cv2.imshow('Landmarks', plots)\n",
    "        cv2.waitKey(20)\n",
    "        \n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
